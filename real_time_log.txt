
train log：

train: 
(26846, 128, 128, 3)
test: 
(3255, 128, 128, 3)
21476 train samples
5370 valid samples
3255 test samples
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 128, 128, 32)      896       
_________________________________________________________________
activation_1 (Activation)    (None, 128, 128, 32)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 126, 126, 32)      9248      
_________________________________________________________________
activation_2 (Activation)    (None, 126, 126, 32)      0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 63, 63, 32)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 63, 63, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 63, 63, 64)        18496     
_________________________________________________________________
activation_3 (Activation)    (None, 63, 63, 64)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 61, 61, 64)        36928     
_________________________________________________________________
activation_4 (Activation)    (None, 61, 61, 64)        0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 30, 30, 64)        0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 30, 30, 64)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 57600)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               29491712  
_________________________________________________________________
activation_5 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 3)                 1539      
_________________________________________________________________
activation_6 (Activation)    (None, 3)                 0         
=================================================================
Total params: 29,558,819
Trainable params: 29,558,819
Non-trainable params: 0
_________________________________________________________________
E:\cs\Anaconda3\lib\site-packages\keras\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 21476 samples, validate on 5370 samples
Epoch 1/40
21476/21476 [==============================] - 248s - loss: 0.1759 - acc: 0.9312 - val_loss: 0.0276 - val_acc: 0.9952
Epoch 2/40
21476/21476 [==============================] - 247s - loss: 0.0226 - acc: 0.9921 - val_loss: 0.0023 - val_acc: 0.9993
Epoch 3/40
21476/21476 [==============================] - 246s - loss: 0.0106 - acc: 0.9963 - val_loss: 0.0019 - val_acc: 0.9994
Epoch 4/40
21476/21476 [==============================] - 248s - loss: 0.0070 - acc: 0.9974 - val_loss: 4.1001e-04 - val_acc: 0.9998
Epoch 5/40
21476/21476 [==============================] - 246s - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0016 - val_acc: 0.9994
Epoch 6/40
21476/21476 [==============================] - 245s - loss: 0.0021 - acc: 0.9993 - val_loss: 2.5290e-04 - val_acc: 0.9998
Epoch 7/40
21476/21476 [==============================] - 245s - loss: 0.0029 - acc: 0.9992 - val_loss: 1.4446e-04 - val_acc: 1.0000
Epoch 8/40
21476/21476 [==============================] - 245s - loss: 0.0014 - acc: 0.9996 - val_loss: 1.2323e-04 - val_acc: 1.0000
Epoch 9/40
21476/21476 [==============================] - 244s - loss: 0.0013 - acc: 0.9994 - val_loss: 3.1777e-04 - val_acc: 0.9998
Epoch 10/40
21476/21476 [==============================] - 245s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0012 - val_acc: 0.9996
Epoch 11/40
21476/21476 [==============================] - 245s - loss: 0.0069 - acc: 0.9980 - val_loss: 0.0013 - val_acc: 0.9996
Epoch 12/40
21476/21476 [==============================] - 246s - loss: 0.0028 - acc: 0.9993 - val_loss: 0.0017 - val_acc: 0.9994
Epoch 13/40
21476/21476 [==============================] - 246s - loss: 8.6676e-04 - acc: 0.9998 - val_loss: 9.6670e-05 - val_acc: 1.0000
Epoch 14/40
21476/21476 [==============================] - 244s - loss: 1.8774e-04 - acc: 1.0000 - val_loss: 1.8998e-04 - val_acc: 0.9998
Epoch 15/40
21476/21476 [==============================] - 245s - loss: 2.8935e-05 - acc: 1.0000 - val_loss: 6.6648e-05 - val_acc: 1.0000
Epoch 16/40
21476/21476 [==============================] - 245s - loss: 4.4198e-05 - acc: 1.0000 - val_loss: 4.5922e-05 - val_acc: 1.0000
Epoch 17/40
21476/21476 [==============================] - 247s - loss: 3.1198e-05 - acc: 1.0000 - val_loss: 6.6971e-05 - val_acc: 1.0000
Epoch 18/40
21476/21476 [==============================] - 243s - loss: 1.3023e-04 - acc: 0.9999 - val_loss: 8.2088e-04 - val_acc: 0.9996
Epoch 19/40
21476/21476 [==============================] - 243s - loss: 3.2421e-05 - acc: 1.0000 - val_loss: 2.5159e-04 - val_acc: 0.9998
Epoch 20/40
21476/21476 [==============================] - 244s - loss: 2.3359e-04 - acc: 0.9999 - val_loss: 2.4653e-05 - val_acc: 1.0000
Epoch 21/40
21476/21476 [==============================] - 244s - loss: 8.6744e-04 - acc: 0.9997 - val_loss: 1.2193e-04 - val_acc: 1.0000
Epoch 22/40
21476/21476 [==============================] - 245s - loss: 1.3345e-04 - acc: 1.0000 - val_loss: 1.8904e-05 - val_acc: 1.0000
Epoch 23/40
21476/21476 [==============================] - 244s - loss: 5.5460e-05 - acc: 1.0000 - val_loss: 1.6217e-05 - val_acc: 1.0000
Epoch 24/40
21476/21476 [==============================] - 243s - loss: 2.8426e-05 - acc: 1.0000 - val_loss: 1.1603e-05 - val_acc: 1.0000
Epoch 25/40
21476/21476 [==============================] - 242s - loss: 1.7604e-05 - acc: 1.0000 - val_loss: 1.2030e-05 - val_acc: 1.0000
Epoch 26/40
21476/21476 [==============================] - 244s - loss: 1.0226e-05 - acc: 1.0000 - val_loss: 1.1637e-05 - val_acc: 1.0000
Epoch 27/40
21476/21476 [==============================] - 243s - loss: 1.9865e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 0.9996
Epoch 28/40
21476/21476 [==============================] - 244s - loss: 3.8532e-04 - acc: 0.9999 - val_loss: 7.8919e-05 - val_acc: 1.0000
Epoch 29/40
21476/21476 [==============================] - 244s - loss: 1.6538e-04 - acc: 1.0000 - val_loss: 1.6280e-05 - val_acc: 1.0000
Epoch 30/40
21476/21476 [==============================] - 245s - loss: 7.6319e-05 - acc: 1.0000 - val_loss: 3.2736e-06 - val_acc: 1.0000
Epoch 31/40
21476/21476 [==============================] - 244s - loss: 1.3372e-05 - acc: 1.0000 - val_loss: 6.2783e-06 - val_acc: 1.0000
Epoch 32/40
21476/21476 [==============================] - 244s - loss: 1.4205e-05 - acc: 1.0000 - val_loss: 3.0604e-05 - val_acc: 1.0000
Epoch 33/40
21476/21476 [==============================] - 244s - loss: 1.1831e-05 - acc: 1.0000 - val_loss: 1.9413e-05 - val_acc: 1.0000
Epoch 34/40
21476/21476 [==============================] - 244s - loss: 1.2772e-04 - acc: 1.0000 - val_loss: 2.6531e-06 - val_acc: 1.0000
Epoch 35/40
21476/21476 [==============================] - 244s - loss: 4.1247e-04 - acc: 0.9998 - val_loss: 6.7489e-05 - val_acc: 1.0000
Epoch 36/40
21476/21476 [==============================] - 244s - loss: 0.0010 - acc: 0.9996 - val_loss: 2.5726e-06 - val_acc: 1.0000
Epoch 37/40
21476/21476 [==============================] - 244s - loss: 2.3757e-05 - acc: 1.0000 - val_loss: 7.6915e-07 - val_acc: 1.0000
Epoch 38/40
21476/21476 [==============================] - 246s - loss: 1.4745e-04 - acc: 0.9999 - val_loss: 0.0013 - val_acc: 0.9994
Epoch 39/40
21476/21476 [==============================] - 246s - loss: 0.0012 - acc: 0.9998 - val_loss: 2.8008e-05 - val_acc: 1.0000
Epoch 40/40
21476/21476 [==============================] - 241s - loss: 3.5608e-04 - acc: 0.9999 - val_loss: 3.4127e-05 - val_acc: 1.0000



test log：

21476 train samples
5370 valid samples
3255 test samples
3255/3255 [==============================] - 11s     
acc: 100.00%